---
title: "Gather Relevant Tweets using the Hatebase set of words"
output: html_notebook
---

```{r load libraries}
library(httr)
library(tidyverse)
library(xml2)
library(rtweet)
```


# Download hatebase terms and filter to those who's listing meaning contains the word 'black'
```{r get terms}
key <- read_file('resources/hatebasekey.txt') # read stored api key
terms <- data_frame(terms=character(), meanings=character()) # empty df for terms
for(page in 1:5){ # loop through number of pages
  request_url = paste0('https://api.hatebase.org/v3-0/', key, '/vocabulary/xml/about_ethnicity=1%7clanguage=eng%7cpage=',page) # make api call url
  request <- GET(request_url) # GET api results
  content <- read_xml(content(request, type='text')) # process xml
  tmp <- data_frame(terms = unlist(as_list(xml_find_all(content, xpath = '//vocabulary'))), # pull out the resulting terms
           meanings = unlist(as_list(xml_find_all(content, xpath = '//meaning')))) # and their meanings
  terms <- bind_rows(terms, tmp[str_detect(tmp$meanings, 'black'),]) # only keep terms which mention 'black' in their meaning field
}

```

Add Rene's words to make a full list of terms
```{r edit terms}
# terms from Flores 2017
flores_terms <- c('African-American', 'blacks', 'AfricanAmerican', 'black people', 'black ppl', 'black guy', 'black girl', 'black dude', 'black men', 'black female', 'black man', 'black male', 'black women')
# joined with ours for a full list
term_list <- append(terms$terms, flores_terms)
#turns out those terms include lots of ambiguous terms so I edited them by hand
term_list <- read_lines('resources/edited_term_list_2') #quotes are necessary
```

# Grab tweets that match those terms
```{r get tweets}
# grab n tweets matching each of the terms in our list
n = 200
tweet_sample <- search_tweets2(q=term_list, n=n, include_rts = FALSE, lang = "en")
# make a search term column and remove duplicate texts
tweet_sample <- tweet_sample %>% mutate(search_term = str_replace(rownames(tweet_sample), '\\..+','')) %>% distinct(text, .keep_all = TRUE)
write_as_csv(tweet_sample, 'data/tweet_sample6_27.csv', prepend_ids = FALSE) # save sample as csv
limited <- new_sample %>% select(text, search_term)

#make a random subset of 100 tweets

sub_sample_100 <- tweet_sample %>% sample_n(100)
sub_sample_100['index'] <- rownames(sub_sample_100)
write_as_csv(sub_sample_100, 'data/sub_sample_100.csv', prepend_ids = FALSE) # save sample as csv
#### load sample from csv
# tweet_sample <- read_csv('data/tweet_sample6_26.csv', 
#          col_types = cols(user_id = col_character(), 
#                           status_id = col_character())) #make sure to maintain user_id and status_id as character strings!
```


```{r adjust sample}
ians_ratings <- read_csv("data/ian's ratings.csv", col_types = cols(id = col_character()))
non_zero <- ians_ratings %>% filter(rating!=0)
non_zero_full <- semi_join(sub_sample_100,non_zero, by = c('status_id' = 'id'))
new_sample <- tweet_sample %>% sample_n(56) %>% bind_rows(non_zero_full) %>% sample_n(100)
new_sample['index'] <- 1:100
write_as_csv(sample_n(new_sample,100), 'data/test_tweets.csv', prepend_ids = FALSE)
current_sample <- read_csv('data/test_tweets.csv', 
         col_types = cols(user_id = col_character(),
                          status_id = col_character()))
```

```{r}
files <- list.files('data/responses')
ratings = data_frame(
  index = col_integer(),
  user = col_character(),
  rating = col_integer(),
  topic = col_character(),
  notes = col_character(),
  id = col_character(),
  screenname = col_character(),
  timestamp = col_character()
)
for(file in files){
  tmp <- read_csv(paste0('data/responses/',file))
  ratings <- rbind(ratings, tmp)
}
ratings <- ratings %>% filter(user!="rox chiappa") %>% mutate(rating_sign = sign(rating))
means <- ratings %>% select(index, user, rating, rating_sign) %>% group_by(index) %>% summarise(mean = mean(rating), sd = sd(rating), mean_sign = mean(rating_sign))
ratings <- ratings %>% 
  select(index, user, rating, rating_sign) %>% 
  inner_join(means)


ratings %>% mutate(meansqerr = (mean-rating)^2,
                   error_sign = (mean_sign-rating_sign)^2) %>% 
  group_by(index) %>% 
  summarise(error_sign = mean(error_sign)) %>% 
  filter(error_sign ==0)

ratings %>% inner_join(new_sample %>% select(screen_name, text) %>% mutate(index=1:100), by = 'index') %>% write_csv('data/ratings_6_29.csv')
```

```{r}
ratings %>% filter(user!="rox chiappa") %>%
  ggplot(aes(factor(sign(mean)), rating))+
    geom_boxplot(aes(color=user))+
    xlab("Sign of Mean Rating")+
    ylab("User Rating")
```

```{r}
ratings %>% mutate(agree = rating_sign==mean_sign) %>% group_by(user) %>% count(agree)
```

```{r}
```

