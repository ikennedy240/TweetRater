---
title: "Gather Relevant Tweets using the Hatebase set of words"
output: html_notebook
---

```{r load libraries}
library(httr)
library(tidyverse)
library(xml2)
library(rtweet)
library(irr)
library(knitr)
```


# Import term list
```{r get terms}
# see archive/tweetGather_old.rmd for gathering the term list
# this list has the list of terms for black folks from Flores 2017 plus some extra terms 
# which we think might be common anti-black slurs. Once we have a tool for separating out 
# tweets that aren't about black folks, we could try adding more terms
term_list <- read_lines('resources/term_list_3')
```

# Import current full tweet set and add some new tweets
I want to keep a file that includes all the tweets that we've downloaded for the project because those will be the sampling frame from which we've drawn past samples. We can still, if we want, make new samples entirely from new tweets, but I want to make sure that we have the option to remove previously downloaded tweets as well as, of course, removing tweets that were included in past sampling frames.
```{r}
# load previous full set
full_tweet_set <- read_csv('data/tweets/full_tweet_set.csv', 
         col_types = cols(user_id = col_character(),
                          status_id = col_character(),
                          reply_to_status_id = col_character(),
                          reply_to_user_id = col_character(),
                          mentions_user_id = col_character(),
                          quoted_status_id = col_character(),
                          quoted_user_id = col_character(),
                          retweet_status_id = col_character(),
                          retweet_user_id = col_character()))
#tweet_sample %>% select(ends_with('_id')) %>% names()
#mutate(sample_date = ifelse(status_id %in% sub_sample$status_id, '20-07-2018',sample_date)) %>% write_csv('data/tweets/full_tweet_set.csv')
```


```{r get new tweets}
# grab n tweets matching each of the terms in our list
n = 100
tweet_sample <- search_tweets2(q=term_list, n=n, include_rts = FALSE, lang = "en")

# make a search term column and remove duplicate texts
tweet_sample <- tweet_sample %>% 
  mutate(search_term = str_replace(rownames(tweet_sample), '\\..+',''))

# optionally add some user histories
  # load some set of old responses
old_responses <- read_csv('data/ratings/responses18_07_2018.csv', 
                          col_types = cols(user_id = col_character(),
                          status_id = col_character()))
#get the users with the lowest mean ratings
target_sns <- old_responses %>% group_by(screen_name) %>% summarise(mean = mean(mean)) %>% arrange(mean) %>% head(3)

# grab the timelines from those users
low_mean_user_tweets <- get_timelines(target_sns$screen_name, n=500) %>%
  mutate(search_term = screen_name) %>% # add a search term colunm with the screen_name
  filter(str_detect(text,regex('black|nigger|porch|race|bait', ignore_case = T)), is.na(`retweet_created_at`)) #filter to some relavent terms

tweet_sample <- bind_rows(tweet_sample, low_mean_user_tweets) %>% # bind with tweet sample
  distinct(text, .keep_all = TRUE) %>% # drope dupes
  filter(!(status_id %in% full_tweet_set$status_id)) # drop previously collected tweets


# save that sample with the date it was collected
write_as_csv(tweet_sample, 
             paste0('data/tweets/tweet_sample_',format(Sys.time(), "%d_%m_%Y"),'.csv'), 
             prepend_ids = FALSE)

```

```{r get new tweets with term 'racist' in reply to other twees}
reply_sample <- search_tweets(q='racist', n=1000, include_rts = FALSE, lang = "en")
in_reply_to <- reply_sample %>% filter(!is.na(reply_to_screen_name), str_detect(text, 'racist'))
replied_to <- lookup_statuses(in_reply_to$reply_to_status_id, parse = TRUE, token = NULL)
replied_to %>% filter(str_detect(text, regex(paste0(term_list, collapse = '|'))))
replied_to$text
write_as_csv(replied_to, 
             paste0('data/tweets/reply_sample_',format(Sys.time(), "%d_%m_%Y"),'.csv'), 
             prepend_ids = FALSE)
```


```{r make new sample subset}
#make a random subset of k tweets
k = 100
sub_sample <- tweet_sample %>% 
  sample_n(k) %>% 
  mutate(sample_date = format(Sys.time(), "%d_%m_%Y"))
sub_sample['index'] <- 1:k
# save as test_tweets for transfer to rater
write_as_csv(sub_sample, 'data/test_tweets.csv', prepend_ids = FALSE) # save sample as csv

# merge everything into the full set and label the new sample with the date
full_tweet_set <- bind_rows(full_tweet_set, flatten(tweet_sample)) %>% 
  mutate(sample_date = ifelse(status_id %in% sub_sample$status_id, format(Sys.time(), "%d_%m_%Y"),sample_date))
# write the full sample to disk
full_tweet_set %>% write_csv('data/tweets/full_tweet_set.csv')
full_tweet_set %>% count(sample_date)
```


```{r process responses}
# grab the list of files
files <- list.files('data/ratings/hit1')
# dart an empty data frame
responses = data_frame(
  index = col_integer(),
  user = col_character(),
  rating = col_integer(),
  topic = col_character(),
  notes = col_character(),
  status_id = col_character(),
  screen_name = col_character(),
  timestamp = col_character()
)
# read all the files
for(file in files){
  tmp <- read_csv(paste0('data/ratings/hit1/',file), 
                  col_types = cols(status_id = col_character())) %>% mutate(user = substr(file,28,52))
  responses <- rbind(responses, tmp)
}

# calculate mean ratings
responses <- responses  %>% 
  inner_join(turker_tweets %>% select(text, screen_name), by = 'screen_name') %>% 
  mutate(rating = ifelse(valence == 'Anti-Racist'|valence=='Empowering', abs(rating), rating), rating_sign = sign(rating))
means <- responses %>% group_by(index) %>% summarise(mean = mean(rating), sd = sd(rating), mean_sign = mean(rating_sign))
adj_means <- responses %>% inner_join(means) %>% filter(sign(rating)==sign(mean)) %>% group_by(index) %>% summarise(adj_mean = mean(rating))
responses <- responses %>% inner_join(means) %>% inner_join(adj_means)

# look at the mean errors
responses %>% mutate(meansqerr = (mean-rating)^2,
                   error_sign = (mean_sign-rating_sign)^2) %>% 
  group_by(index) %>% 
  summarise(error_sign = mean(error_sign), meansqerr = mean(meansqerr)) %>% 
  arrange(desc(error_sign))

responses %>% group_by(user) %>% count()
cat(turker_tweets$text[[22]])

responses %>% mutate(rating = ifelse(valence == 'Anti-Racist', abs(rating), rating)) %>% select(index,user, valence, rating_sign, mean) %>% filter(index==22)


# save responses to disk
responses %>%
  write_csv(paste0('data/ratings/responses', format(Sys.time(), "%d_%m_%Y"),'.csv'))
```

```{r}
responses %>% 
  ggplot(aes(factor(sign(mean)), rating))+
    geom_boxplot(aes(color=race_ethnicity))+
    xlab("Sign of Mean Rating")+
    ylab("User Rating")+
  theme(legend.title=element_blank())
```

```{r}
ratings_report <- function(responses){
  # text and mean rating for all tweets
  print(kable(responses %>% group_by(index) %>% summarise(text = substr(str_remove_all(first(text),'\\n'),1,50), mean = first(mean), adj_mean = round(first(adj_mean),1))))
  # text and detailed rating for each tweet
  for(i in 1:length(unique(responses$index))){
    print(paste("Tweet number", i, "had an average rating of", responses$mean[[i]], "and an adjusted average of",responses$adj_mean[[i]]))
    cat('\n\n\n')
    cat(responses$text[[i]])
    print(kable(responses %>% filter(index == i) %>% mutate(user = factor(user, labels = c(1:10))) %>% select(user:state, valence:topic)))
    cat('\n\n\n')
  }
}
```


```{r}

capture.output(ratings_report(responses), file = 'ratings_report.txt', type = 'output')
```


```{r}
irr_responses <- responses %>% 
  filter(sd<1.5) %>%
  select(user, rating, index) %>%
  mutate(rating = sign(rating)) %>%
  spread(user, rating) %>% 
  select(-index)
x <- kappam.fleiss(irr_responses)
irr_responses %>% 
  select(ian, Savannah) %>% 
  kappa2(weight = 'squared')

test_tweets <- rated_set %>% filter(topic != 'NULL') %>% group_by(status_id) %>% summarise(screen_name = first(screen_name), rating = mean(rating)) %>% inner_join(full_tweet_set, by = 'status_id')


#write_csv(test_tweets, 'data/tweets/test_tweets.csv')
kable(responses %>% group_by(index) %>% summarize(sd = sd(rating)) %>% arrange(desc(sd)))
summary(responses$sd)
x$value
```

Construct list for turkers
```{r}
list <- read_csv('data/tweets/racist_tweets.csv') %>% transmute(status_id = as.character(status_id))
list <- responses %>% filter(example == 'TRUE') %>% select(status_id) %>% bind_rows(list)

turker_tweets <- lookup_statuses(list$status_id, parse = TRUE)

write_csv(flatten(turker_tweets) %>% sample_frac(1), "test_tweets.csv")

old_responses %>% group_by(status_id) %>% summarise_if(is.numeric, mean) %>% arrange(rating) %>% filter(rating>-.67)


full_tweet_set[full_tweet_set$status_id=='1015970747712405505',]$text
```

```{r}
for(i in 1:length(turker_tweets$text)){
  cat(paste("\n\n Example Tweet #",as.character(i), '\n'))
  cat(turker_tweets$text[25-i])
}
View(responses %>% filter(user=='c4009c1b18f5e0992b8de97f3') %>% select(valence, rating, mean, text))
```

